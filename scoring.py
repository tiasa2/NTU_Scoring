!pip install torch==1.5.1
!pip install transformers==3.0.1
!pip install -U sentence-transformers
!pip install sentence-splitter
!pip install SentencePiece
import scipy
from sentence_transformers import SentenceTransformer

import json
f = open('') #change this to link any pegasus output file
data_pega = json.load(f)

import nltk
nltk.download('punkt')
from nltk.tokenize import word_tokenize

def tokenize_data(sent_list):
  data = []
  # iterate through each sentence in the file
  for i in sent_list:
      temp = []
      # tokenize the sentence into words
      for j in word_tokenize(i):
          temp.append(j.lower())
      data.append(temp)
  return data

def cand_ref(sent_list,text):
  refs = sent_list
  cands = [text] * len(refs)
  return cands,refs

"""

>BERT_Score

"""

!pip install bert_score==0.3.4

from bert_score import BERTScorer
import torch
device = 'cuda' if torch.cuda.is_available() else 'cpu'

import gc
def compute_bert_score(cands, refs):
  gc.collect()
  scorer = BERTScorer(lang="en")
  P, R, F1 = scorer.score(cands, refs)
  sent_score_bert = []
  for i in range(0,len(refs)):
    sent_score_bert.append((refs[i],F1.tolist()[i]))  
  del P
  del R
  del F1
  sent_score_bert.sort(key = lambda x: x[1])
  sent_score_bert.reverse()
  return sent_score_bert

def compute_semantic_similarity(data,sent_list):
  model = SentenceTransformer('bert-base-nli-mean-tokens')
  model.to(device)
  ref_embeddings = model.encode(sent_list)
  cand_embeddings = model.encode(data)
  distances = []
  for query, query_embedding in zip(sent_list, ref_embeddings):
      distance = scipy.spatial.distance.cdist([query_embedding], [cand_embeddings], "cosine")[0]
      distances.append((query,distance.tolist()[0]))
  distances.sort(key = lambda x: x[1])
  return distances

# f1_scores = []
# for i in range(0,len(sent_list)):
#   P, R, F1 = score(cands, refs, lang="en", verbose=True, device = device)
#   f1_scores.append(F1)

# f1_scores_list = []
# for i in f1_scores:
#   f1_scores_list.append(i.tolist()[0])

# sent_score_bert = []
# for i in range(0,len(sent_list)):
#   sent_score_bert.append((sent_list[i],f1_scores_list[i]))

# with open('/content/drive/MyDrive/'+c+'/bert_score.txt', 'w+') as f:
#   for items in sent_score_bert:
#     f.write('%s\n' %str(items))
#   print("File written successfully")
# f.close()

"""

> SELF-BLEU

"""

from nltk.translate.bleu_score import sentence_bleu
from nltk.translate.bleu_score import SmoothingFunction
import numpy as np
import copy

def get_bleu_score(sentence, remaining_sentences):
    lst = []
    smoothie = SmoothingFunction().method4
    # for i in remaining_sentences:
    bleu = sentence_bleu(remaining_sentences, sentence, smoothing_function=smoothie)
    # lst.append(bleu)
    return bleu


def calculate_selfBleu(sentences):
    '''
    sentences - list of sentences generated by NLG system
    '''
    bleu_scores = []
	
    for i in sentences:
        sentences_copy = copy.deepcopy(sentences)
        remaining_sentences = sentences_copy.remove(i)
        bleu = get_bleu_score(i,sentences_copy)
        bleu_scores.append(bleu)

    return [np.mean(bs) for bs in bleu_scores]

def compute_self_bleu(data,sent_list): 
  scores = calculate_selfBleu(data)
  sent_score = []
  for i in range(0,len(sent_list)):
    sent_score.append((sent_list[i],scores[i]))
  sent_score.sort(key = lambda x: x[1])
  return sent_score

# with open('/content/drive/MyDrive/'+c+'/self_bleu.txt', 'w+') as f:
#   for items in sent_score:
#     f.write('%s\n' %str(items))
#   print("File written successfully")
# f.close()

"""

> Levenstein

"""

!pip install Levenshtein

import Levenshtein

import numpy as np
import copy

def get_Levenshtein(sentence, remaining_sentences):
    lst = []
    for i in remaining_sentences:
        lev = Levenshtein.distance(sentence, i)
        lst.append(lev)
    return lst


def calculate_selfLevenshtein(sentences):
    '''
    sentences - list of sentences generated by NLG system
    '''
    lev_scores = []
	
    for i in sentences:
        sentences_copy = copy.deepcopy(sentences)
        remaining_sentences = sentences_copy.remove(i)
        lev = get_Levenshtein(i,sentences_copy)
        lev_scores.append(lev)

    return [np.mean(ls) for ls in lev_scores]

def compute_selfLevenshtein(sent_list):
  scores = calculate_selfLevenshtein(sent_list)
  sent_score_new = []
  for i in range(0,len(sent_list)):
   sent_score_new.append((sent_list[i],scores[i]))
  sent_score_new.sort(key = lambda x: x[1])
  sent_score_new.reverse()
  return sent_score_new

# with open('/content/drive/MyDrive/'+c+'/levenshtein.txt', 'w+') as f:
#   for items in sent_score_new:
#     f.write('%s\n' %str(items))
#   print("File written successfully")
# f.close()

"""

> avg_index

"""

def compute_avg_index(sent_score_bert,sent_score_new,sent_list):
  #for context
  sent_score_bert_sent = []
  for i in sent_score_bert:
    sent_score_bert_sent.append(i[0])
  #for diversity
  sent_score_sent = []
  for i in sent_score_new:
    sent_score_sent.append(i[0])
  avg_index = []
  for i in sent_list:
    avg_index.append((i,sent_score_bert_sent.index(i)+sent_score_sent.index(i)/2))
  avg_index.sort(key = lambda x: x[1])
  return avg_index

# with open('/content/drive/MyDrive/'+c+'/avg_index_lev.txt', 'w+') as f:
#   for items in avg_index:
#     f.write('%s\n' %str(items))
#   print("File written successfully")
# f.close()

"""

> greedy selection

"""

def get_bleu_score_greedy(sentence, remaining_sentences):
    smoothie = SmoothingFunction().method4
    lst = []
    for i in remaining_sentences:
        bleu = sentence_bleu(sentence, i, smoothing_function=smoothie, weights = (0.1,0.1,0.1,0.7))
        lst.append(bleu)
    return lst

def compute_final(avg_index):
  texts = []
  texts.append(avg_index[0][0])
  for j in range(0,4):
    avg_index_sent = []
    for i in avg_index[1:25]:
      scores_final = get_Levenshtein(i[0],texts)
      avg_index_sent.append((i[0],np.mean(scores_final)))
    avg_index_sent.sort(key = lambda x: x[1])
    avg_index_sent.reverse()
    for i in avg_index_sent:
      if i[0] not in texts:
        texts.append(i[0])
        break
  return texts

d = {}
count = 1

for key, value in data_pega.items():
  count += 1
  text = key
  sent_list = value
  data = tokenize_data(sent_list)
  cands, refs = cand_ref(sent_list,text)
  sent_score_bert = compute_semantic_similarity(text, sent_list)
  sent_score_new = compute_selfLevenshtein(sent_list)
  avg_index = compute_avg_index(sent_score_bert,sent_score_new,sent_list)
  final_texts = compute_final(avg_index)
  d[key] = final_texts


json_object = json.dumps(d, indent = 0)
with open('', 'w') as outfile: #output file to check
    outfile.write(json_object)

